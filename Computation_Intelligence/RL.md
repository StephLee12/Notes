- [Reinforcement Learning æå®æ¯…](#reinforcement-learning-%e6%9d%8e%e5%ae%8f%e6%af%85)
  - [Policy Gradient](#policy-gradient)
    - [Basic Components](#basic-components)
    - [Goal](#goal)
    - [Policy Gradient Part](#policy-gradient-part)
    - [Implementation](#implementation)
  - [Proximal Policy Optimization(PPO)](#proximal-policy-optimizationppo)
    - [On-policy to Off-policy](#on-policy-to-off-policy)
    - [Importance Sampling](#importance-sampling)
    - [Implement Off-policy](#implement-off-policy)
    - [PPO](#ppo)
  - [Q-Learning](#q-learning)
    - [State-Value Function](#state-value-function)
    - [State-Action Value Funtionâ€”â€”Q Function $\,Q^\pi (s,a)\,$](#state-action-value-funtionq-function-mathsemanticsmrowmtext-mtextmsupmiqmimi%cf%80mimsupmo-stretchy%22false%22momismimo-separator%22true%22momiamimo-stretchy%22false%22momtext-mtextmrowannotation-encoding%22applicationx-tex%22qpi-saannotationsemanticsmathq%cf%80sa)
    - [Tricks](#tricks)
    - [Q-Learning Algorithm](#q-learning-algorithm)

# Reinforcement Learning æå®æ¯…

## Policy Gradient

### Basic Components

ğŸ‘‰ Agentâ€”â€”éœ€è¦make policy

ğŸ‘‰ ç¯å¢ƒå’ŒReward Functionâ€”â€”éƒ½æ˜¯å·²ç»è®¾å®šå¥½çš„ï¼Œä¸èƒ½ä¿®æ”¹

ğŸ‘‰ Policy(é€šå¸¸ç”¨$\,\pi\,$è¡¨ç¤º)æ˜¯ä¸€ä¸ªå«å‚$\,\theta\,$çš„ç½‘ç»œ

ğŸ‘‰ Policyçš„è¾“å…¥é€šå¸¸æ˜¯**æœºå™¨çš„ç”»é¢(å¦‚æ¸¸æˆæŸæ—¶åˆ»çš„ç”»é¢)**ï¼Œè¾“å…¥æ˜¯ç”¨çŸ©é˜µçš„å½¢å¼è¾“å…¥(å°†ç”»é¢æŠ½è±¡ä¸ºåƒç´ )

ğŸ‘‰ Policyçš„è¾“å‡ºæ˜¯**æ‰§è¡ŒæŸä¸€åŠ¨ä½œçš„æ¦‚ç‡**ï¼ŒAgentä¼šæ ¹æ®è¿™ä¸ªæ¦‚ç‡æ¥åšå‡ºå¯¹åº”çš„Action

We define some symbols:

æŸä¸€çŠ¶æ€(å¯¹äºæ¸¸æˆæ¥è¯´æ˜¯ç”»é¢)$\,\rightarrow\, s_i$

reward $\,\rightarrow r_i\,$

action $\,\rightarrow a_i\,$

### Goal

ğŸ‘‰$\,R = \sum_{i=1}^T r_i\,$ï¼Œç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´Policyçš„å‚æ•°$\,\theta\,$ä½¿$\,R\,$æœ€å¤§åŒ–

![Relationship among Actor,Env,Reward](Captures\RL1.PNG "Relationship among Actor,Env,Reward")

ğŸ‘‰æ¯ä¸€åœºæ¸¸æˆéƒ½æœ‰ä¸€ä¸ª**trajectory**
$$
    \tau = \{ s_1,a_1,s_2,a_2,\cdots,s_T,a_T \}
$$
ğŸ‘‰æŸä¸€è½¨è¿¹å‡ºç°çš„æ¦‚ç‡è®¡ç®—å¦‚ä¸‹
$$
    \begin{aligned} &
        p_\theta(\tau)\\ &
        =p(s_1)p_\theta(a_1 \mid s_1)p_(s_2\mid s_1,a_1)p_\theta(a_2\mid s_2)p(s_3|s_2,a_2)\cdots \\ &
        =p(s_1)\prod_{t=1}^T p_\theta(a_t\mid s_t)p(s_{t+1}\mid s_t,a_t)
    \end{aligned}
$$
è¦æ³¨æ„åˆ°$\,R\,$æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå› ä¸º$\,s_i,a_i\,$å…·æœ‰éšæœºæ€§ã€‚è‹¥è¦æœ€å¤§åŒ–$\,R\,$ï¼Œåˆ™è¦æœ€å¤§åŒ–$\,R\,$çš„æœŸæœ›

ğŸ‘‰ä¸‹é¢æ˜¯è®¡ç®—$\,R\,$çš„æœŸæœ›çš„æ¢¯åº¦(æœ‰æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ€æƒ³)
$$
    \begin{aligned} &
        \nabla \,\,\overline{R_\theta} \\
        &= \sum_\tau R(\tau)\nabla p_\theta(\tau) \\
        & =\sum_\tau R(\tau) p_\theta(\tau) \frac{\nabla p_\theta(\tau)}{p_\theta(\tau)} \\
        &= \sum_\tau R(\tau) p_\theta(\tau)  \nabla \log p_\theta(\tau) \\
        &= E_{\tau \sim p_\theta(\tau)} [R(\tau)\nabla\log p_\theta(\tau)] \\
        &\approx \frac{1}{N} \sum_{n=1}^N R(\tau^n)\nabla\log p_\theta(\tau^n) \,\,\text{note that n is index and we need to use sampling to compute} \\
        &= \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n)\nabla
        \log p_\theta(a_t^n \mid s_t^n) \\
        &\text{In the last formula,note that in formula} \,p_\theta(\tau) \text{we cannot control} \,p(s_1) \text{because it's based on the Env so we only need to "gradient" the part with} \,\theta
    \end{aligned}
$$
å³è¦ä¼˜åŒ–çš„$\,R\,$çš„æœŸæœ›ä¸º(å¾ˆåƒæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œ$\,p_\theta\,$åƒæ¦‚ç‡å¯†åº¦å‡½æ•°)
$$
    \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n)
        \log p_\theta(a_t^n \mid s_t^n)
$$

**i.e** There is a formula for the above demonstration:
$$
    \nabla f(x) = f(x) \nabla \log f(x)
$$

### Policy Gradient Part

![Compute Policy Gradient](Captures\RL2.PNG "Compute Policy Gradient")

è®­ç»ƒå¥½çš„Agentï¼Œè¿›è¡Œæ•°æ®çš„é‡‡é›†ï¼Œå¸¦å…¥å…¬å¼è®¡ç®—ä¹‹åè°ƒæ•´å‚æ•°å¾—åˆ°æ–°çš„æ¨¡å‹ï¼Œå†æ¬¡è¿›è¡Œæ•°æ®é‡‡é›†ï¼Œä¸æ–­è°ƒå‚

ğŸ‘‰æ„Ÿè§‰sampleè¦èŠ±å¥½ä¹…ï¼Œè€Œä¸”updateä¸€æ¬¡å°±ä¸èƒ½ç”¨äº†

### Implementation

ğŸ‘‰è€ƒè™‘ä¸ºä¸€ä¸ª**åˆ†ç±»(classification)** çš„é—®é¢˜

![classfication](Captures/RL3.PNG "classfication")

inputæ˜¯æŸä¸€state$\,s_t^n,$çš„ç•Œé¢ï¼Œoutputæ˜¯è¿™ä¸ªstateä¸‹å¯¹åº”çš„action$\,a_t^n\,$,ä¸ºäº†ä¼˜åŒ–
$$
    \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n)
        \log p_\theta(a_t^n \mid s_t^n)
$$
å³å‘Šè¯‰agentå½“é‡åˆ°åŒæ ·çš„state$\,s_t^n,$æ—¶ï¼Œè¾“å‡ºå¯¹åº”çš„action$\,a_t^n\,$ï¼Œå¯¹åº”åˆ°å…¬å¼ä¸Šå³ä¸ºæ±‚æ¢¯åº¦,å³æ‹‰é«˜é‡åˆ°state$\,s_t^n,$æ—¶ï¼Œè¾“å‡ºaction$\,a_t^n\,$çš„æ¦‚ç‡ã€‚ä½†æ³¨æ„ï¼Œrewardæ˜¯**æ•´åœºæ¸¸æˆçš„reward**
$$
    \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} R(\tau^n)\nabla
        \log p_\theta(a_t^n \mid s_t^n)
$$

ä½†æ˜¯åœ¨Implementæ—¶ä¼šé‡åˆ°é—®é¢˜ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º

![baseline](Captures/RL4.PNG "baseline")

ä¸€èˆ¬æ¥è¯´rewardéƒ½æ˜¯æ­£çš„ï¼Œå‡è®¾åœ¨state$\,s_t^n,$å¯ä»¥æ‰§è¡Œä¸‰ä¸ªactionâ€”â€”a,b,cã€‚æ‰€ä»¥æ‰§è¡Œa,b,cçš„æ¦‚ç‡éƒ½ä¼šå¢åŠ ï¼Œä½†æ˜¯ä¸åŒçš„actionçš„rewardä¸åŒï¼Œæ‰€ä»¥ç»è¿‡normalizationä¹‹åï¼Œrewardå°çš„ï¼Œå¦‚å›¾ä¸­çš„bçš„æ¦‚ç‡å°±ä¼šä¸‹é™

ä½†æ˜¯åœ¨**Sampling**æ—¶ç”±äºæ•°æ®ä¸èƒ½è¦†ç›–çš„å…¨éƒ¨çš„å¯èƒ½ï¼Œå¯èƒ½ä¼šå‡ºç°å¦‚action aæ²¡æœ‰è¢«samplingåˆ°ï¼Œä½†ä¸èƒ½è¯´æ˜action açš„rewardå°±å¾ˆå°ï¼Œä½†å› ä¸ºæ²¡æœ‰samplingåˆ°ï¼Œaction bå’Œcçš„æ¦‚ç‡å°±ä¼šå¢å¤§ï¼Œaction açš„æ¦‚ç‡å°±ä¼šå‡å°ï¼Œè¿™æ ·å°±å‡ºç°äº†é—®é¢˜

ğŸ‘‰æ‰€ä»¥è¦è®¾ç½®ä¸€ä¸ª**baseline**ï¼Œå½“ä¸€ä¸ªactionçš„rewardå¾ˆå°çš„æ—¶å€™ï¼Œè¦é™ä½æ‰§è¡Œæ­¤actionçš„æ¦‚ç‡ï¼Œå¼•å…¥
$$
    b \approx E[R(\tau)]
$$
æ‰€ä»¥$\,\nabla \overline{R_\theta}\,$çš„å…¬å¼è¦æ”¹å†™ä¸º
$$
    \nabla \overline{R_\theta} \approx
    \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} (R(\tau^n)-b)\nabla
        \log p_\theta(a_t^n \mid s_t^n)
$$

æ­¤å¤–ï¼Œæ¯ä¸€ä¸ªstateéƒ½è¦ä¹˜ä¸ŠåŒæ ·çš„reward$\,(R(\tau^n)-b)\,$,è¿™æ ·å…¶å®æ˜¯ä¸å…¬å¹³çš„ï¼Œæ¯ä¸€ä¸ªstateä¹˜ä¸Šçš„rewardåº”è¯¥æ˜¯è¿™ä¸ªstateä¹‹å(åŒ…æ‹¬è¿™ä¸ªstate)äº§ç”Ÿçš„rewardä¹‹å’Œï¼Œä½†ä¸¤ä¸ªstateä¹‹é—´çš„æ—¶é—´é—´éš”è¶Šé•¿ï¼Œåé¢çš„stateçš„rewardå¯¹è¯¥stateçš„å½±å“å°±ä¼šè¶Šå°ï¼Œå› æ­¤è¿˜éœ€è¦ä¹˜ä¸Šä¸€ä¸ªdiscountï¼Œæ‰€ä»¥$\,R(\tau^n)\,$åº”è¯¥å˜ä¸ºä¸‹å¼
$$
    \sum_{t^{'}}^{T_n} \gamma^{t'-t} r_{t'} ^n, \,\, \gamma < 1
$$
å°†
$$
    \sum_{t^{'}}^{T_n} \gamma^{t'-t} r_{t'} ^n -b
$$
ç§°ä¸ºAdvantage Function $\,A^\theta(s_t,a_t)\,$,å®ƒèƒ½å¤Ÿè¡¨ç°å‡ºåœ¨state$\,s_t\,$ï¼Œaction$\,a_t\,$ç›¸è¾ƒäºå…¶ä»–çš„actionæœ‰å¤šå¥½ï¼Œå› ä¸ºå‡å»äº†ä¸€ä¸ªbï¼Œæ‰€ä»¥å®ƒæ˜¯ä¸€ä¸ªç›¸å¯¹å€¼

ç»¼ä¸Š
$$
    \nabla \overline{R_\theta} \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} A^\theta(s_t,a_t)\nabla
        \log p_\theta(a_t^n \mid s_t^n)
$$

## Proximal Policy Optimization(PPO)

ğŸ‘‰ Proximal: è¿‘ç«¯çš„

### On-policy to Off-policy

ğŸ‘‰ On-policy: Agentä¸€è¾¹ä¸ç¯å¢ƒäº’åŠ¨ä¸€è¾¹å­¦ä¹ 

ğŸ‘‰ Off-policy: Agentçœ‹ç€åˆ«äººä¸ç¯å¢ƒäº’åŠ¨å­¦ä¹ 

ğŸ‘‰ ä¹‹å‰çš„Policy Gradientå±äºOn-Policyï¼Œæ•ˆç‡å¤ªä½ï¼Œsampleçš„å¾ˆå¤šæ•°æ®åœ¨è¿›è¡Œä¸€æ¬¡model updateä¹‹åå°±ä¸èƒ½ç”¨äº†ï¼Œè¦é‡æ–°sampleã€‚

ğŸ‘‰ Off-policyçš„ç›®æ ‡å°±æ˜¯æœ‰ä¸€ä¸ªæ–°çš„policy$\,\theta^{'}\,$ä¸ç¯å¢ƒåšäº’åŠ¨ï¼Œæ”¶é›†çš„æ•°æ®æ¥è®­ç»ƒpolicy$\,\theta\,$,è¿™æ ·å¯ä»¥å°†$\,\theta^{'}\,$æ”¶é›†çš„æ•°æ®ä½¿ç”¨å¾ˆå¤šæ¬¡

### Importance Sampling

ğŸ‘‰ ISæ˜¯ä¸€ä¸ªgeneralçš„ideaï¼Œä¸æ­¢å¯ä»¥ç”¨äºRL

ğŸ‘‰ ä»ä¸€ä¸ªåˆ†å¸ƒçš„æœŸæœ›åˆ°å¦ä¸€ä¸ªåˆ†å¸ƒçš„æœŸæœ›
$$
    è‹¥f(x)ä¸å¯ç§¯ï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„samplingçš„æ–¹æ³•è®¡ç®—åœ¨x\sim p(x)æ—¶f(x)çš„æœŸæœ›\\
    E_{x\sim p} (f(x)) \approx \frac{1}{N}\sum_{i=1}^{N} f(x^i)
$$
ä½†è‹¥æ— æ³•ä»p(x)ä¸­sample$\,x^i\,$,è€Œåªèƒ½ä»å¦ä¸€ä¸ªåˆ†å¸ƒq(x)ä¸­sampleï¼Œå¯ä»¥é‡‡å–ä¸‹é¢çš„å˜æ¢
$$
    E_{x\sim p} (f(x)) = \int f(x)p(x)dx = \int f(x)\frac{p(x)}{q(x)} q(x)dx = E_{x\sim q} [f(x)\frac{p(x)}{q(x)}] \\
    \frac{p(x)}{q(x)}å¯ä»¥çœ‹ä½œé€šè¿‡q(x)æ¥è®¡ç®—f(x)å¯¹äºp(x)æœŸæœ›çš„ä¿®æ­£æˆ–æƒé‡
$$

ğŸ‘‰ ä½†æ˜¯è¿™ä¸ªæƒé‡ä¸èƒ½ç›¸å·®è¿‡å¤§

å…·ä½“çš„æ•°å­¦æ¨å¯¼å¦‚ä¸‹å›¾æ‰€ç¤º![RL5](Captures\RL5.PNG "RL5")

å½¢è±¡çš„æè¿°å¦‚ä¸‹å›¾æ‰€ç¤º![RL6](Captures\RL6.PNG "RL6")

p(x)åœ¨f(x)<0çš„éƒ¨åˆ†çš„æ¦‚ç‡å¾ˆå¤§ï¼Œæ‰€ä»¥ç­‰å¼å·¦ä¾§çš„å€¼æ˜¯negativeçš„

ä½†æ˜¯å¯¹äºq(x)åˆ™ç›¸åï¼Œè¿™æ ·ä¼šä½¿ç­‰å¼å³ä¾§çš„å€¼æ˜¯positiveçš„ï¼Œä½†æ˜¯å¦‚æœ**sampleçš„æ¬¡æ•°å¤Ÿå¤š**ï¼Œèƒ½å¤Ÿsampleåˆ°f(x)<0çš„éƒ¨åˆ†ï¼Œè¿™æ˜¯$\,\frac{p(x)}{q(x)}\,$çš„å€¼æ˜¯å¾ˆå¤§çš„ï¼Œf(x)å°±ä¼šä¹˜ä¸Šä¸€ä¸ªå¾ˆå¤§çš„æƒé‡(å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„è´Ÿæ•°ï¼Œå¯ä»¥æŠµæ¶ˆsampleåˆ°f(x)>0æ—¶çš„å½±å“)ï¼Œæ‰€ä»¥æ­¤æ—¶æ‰å¯èƒ½ä½¿ç­‰å¼æˆç«‹

### Implement Off-policy

$$
    \nabla \,\,\overline{R_\theta} = E_{\tau \sim p_\theta(\tau)} [R(\tau)\nabla\log p_\theta(\tau)] \Rightarrow \nabla \,\,\overline{R_\theta} = E_{\tau \sim p_{\theta^{'}}(\tau)}[\frac{p_\theta(\tau)}{p_{\theta^{'}}(\tau)} R(\tau)\nabla\log p_\theta(\tau)]
$$
å¼•å…¥ä¸Šæ–‡ä¸­$\,A^\theta(s_t,a_t)\,$ï¼Œgradientå¯æ”¹å†™ä¸ºä¸‹å›¾ä¸­çš„å¼å­![RL7](Captures\RL7.PNG "RL7")

ğŸ‘‰å…¶ä¸­$\,A^\theta(s_t,a_t)\,$è¦å˜ä¸º$\,A^{\theta^{'}}(s_t,a_t)\,$ï¼Œå› ä¸ºä¸ç¯å¢ƒäº¤äº’çš„å¯¹è±¡å‘ç”Ÿäº†å˜åŒ–

ğŸ‘‰backslashå»æ‰çš„é‚£ä¸€é¡¹çš„å€¼å¯ä»¥çœ‹ä½œä¸º1(ä¸¤ä¸ªagentçœ‹åˆ°åŒä¸€ä¸ªç”»é¢çš„æ¦‚ç‡æ—¶ä¸€æ ·çš„ï¼Œæˆ–æŒ‰è§†é¢‘æ‰€è¯´â€”â€”æ²¡æ³•ç®—)

ğŸ‘‰æœ€ç»ˆå¾—åˆ°çš„formulaæ˜¯å–gradientä¹‹å‰çš„objective function(å³è¦optimizeçš„function)

### PPO

ğŸ‘‰ PPOè¦åšçš„äº‹æƒ…å°±æ˜¯é¿å…$\,\theta,\theta^{'}\,$å·®çš„å¤ªå¤§ï¼Œè¦ä¸ç„¶ISçš„ç»“æœä¼šä¸å¥½ ![RL8](Captures\RL8.PNG "RL8")

ğŸ‘‰ åŠ å…¥äº†è®¡ç®—$\,\theta,\theta^{'}\,$KLæ•£åº¦çš„ç½šé¡¹ï¼Œä½†è¿™é‡ŒKLæ•£åº¦è®¡ç®—çš„ä¸æ˜¯ä¸¤ä¸ªå‚æ•°çš„è·ç¦»ï¼Œè€Œæ˜¯å¯¹äºæŸä¸ªstateï¼Œä¸¤ä¸ªactor(å‚æ•°åˆ†åˆ«ä¸º$\,\theta,\theta_{'}\,$)è¾“å‡ºçš„actionçš„è·ç¦»ï¼Œç„¶åå¯¹äºæ‰€æœ‰stateæ±‚å’Œ(åº”è¯¥æ˜¯æ±‚å’Œå§ï¼Œè§†é¢‘é‡Œæ²¡è¯´)

ğŸ‘‰ä¸‹é¢å…¬å¼ä¸­ç¬¬ä¸€ä¸ªcaseï¼Œè¯´æ˜KLæ•£åº¦é¡¹èµ·çš„ä½œç”¨å°ï¼Œç¬¬äºŒä¸ªcaseè¯´æ˜KLæ•£åº¦èµ·çš„ä½œç”¨è¿‡å¤§ï¼Œä½¿$\,\theta ,\theta^k\,$å‡ ä¹ä¸€æ ·äº†

$$
    if KL(\theta ,\theta^k) > KL_{max}, increase \beta\\
    else \quad decrease \beta
$$
ğŸ‘‰ ä¸‹é¢æ˜¯PPOçš„ç¬¬äºŒç§ç®—æ³•![RL9](Captures\RL9.PNG "RL9")
$$
    clipå‡½æ•°æ˜¯æŒ‡è‹¥æƒé‡ < 1- \varepsilon , åˆ™è¯¥å‡½æ•°å–å€¼ä¸º 1- \varepsilon \\
    è‹¥æƒé‡ > 1+ \varepsilon ,è¯¥å‡½æ•°å–å€¼ä¸º 1 + \varepsilon
$$
ğŸ‘‰è‹¥ä»¥æƒé‡ä¸ºæ¨ªè½´ï¼Œclipå‡½æ•°çš„å€¼ä¸ºçºµè½´ï¼Œå‡½æ•°å›¾åƒå¦‚ä¸‹å›¾æ‰€ç¤º![RL10](Captures\RL10.PNG "RL10")

å‰é¢ä¸€é¡¹æ˜¯ä¸€æ¡è¿‡åŸç‚¹çš„ç›´çº¿ï¼Œä¹‹åè¿™ä¸€é¡¹ä¸åé¢çš„clipå–æœ€å°å€¼

ğŸ‘‰å¦‚æœA>0,è¯´æ˜è¦æé«˜$\,p_\theta\,$çš„æ¦‚ç‡ï¼Œä½†æ˜¯æé«˜$\,p_\theta\,$ä¼šå¯¼è‡´å®ƒå’Œ$\,p_{\theta^k}\,$çš„å·®è·å˜å¤§ï¼Œè¿™æ ·ä¸ç¬¦åˆPPOçš„å‡ºå‘ç‚¹ï¼Œå› æ­¤æ­¤æ—¶æœ€å¤§ä¸èƒ½è¶…è¿‡$\,1+\varepsilon\,$ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ˜¯çº¢è‰²çš„çº¿ ![RL11](Captures\RL11.PNG "RL11")

ğŸ‘‰å¦‚æœA<0,ä¸ä¸Šæ–‡åˆšå¥½ç›¸åï¼Œä½†æ˜¯æœ€å°ä¸èƒ½å°è¿‡$\,1-\varepsilon\,$,å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ˜¯çº¢è‰²çš„çº¿ ![RL12](Captures\RL12.PNG "RL12")

## Q-Learning

### State-Value Function

ğŸ‘‰åˆ¤æ–­ä¸€ä¸ªactorçš„å¥½åï¼Œoutput value of a criticå–å†³äºcriticæ‰€è¯„ä»·çš„actor

ğŸ‘‰ State value function $\,V^{\pi} (s)\,$ (å³æ ¹æ®è¾“å…¥çš„stateè¾“å‡ºvalue) 

$\,V^\pi\,$æ˜¯ä¸€ä¸ªç½‘ç»œï¼Œç»™å®šä¸€ä¸ªstateä¹‹åï¼Œä¼šè¾“å‡ºä½¿ç”¨actor$\,\pi\,$ï¼Œåœ¨state sä¹‹åæ‰€èƒ½å¾—åˆ°çš„æ‰€æœ‰çš„rewardä¹‹å’Œ

ğŸ‘‰ estimate $\,V^{\pi} (s)\,$ via Monte-Carlo(MC) based approach

æœé›†å¤§é‡çš„stateï¼Œè®­ç»ƒnetworkï¼Œä½¿å½“çœ‹åˆ°æŸä¸€ä¸ªstateå‡è®¾ä¸º$\,s_a\,$çš„æ—¶å€™ï¼Œè¾“å‡ºä¹‹åçš„rewardæ±‚å’Œ$\,V^\pi (s_a)\,$ï¼Œä½¿è¾“å‡ºçš„å€¼ä¸**æ¸¸æˆç»“æŸæ—¶å¾—åˆ°çš„reward$\,G_a\,$æ˜¯ååˆ†æ¥è¿‘**ï¼Œå¯ä»¥çœ‹ä½œæ˜¯å›å½’

é—®é¢˜æ˜¯ï¼šè¦é‡‡é›†å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒnetworkï¼Œè€Œä¸”æ¯ç›˜æ¸¸æˆç»“æŸåæ‰èƒ½å¾—åˆ°çœŸå®çš„reward,ä½†æœ‰çš„æ¸¸æˆæ—¶é—´å¾ˆé•¿ï¼Œè‹¥è¦æ¸¸æˆç»“æŸï¼Œå¾ˆéš¾é‡‡é›†åˆ°å¾ˆå¤šæ•°æ®ã€‚äºæ˜¯å¼•å‡ºä¸‹é¢çš„æ–¹æ³•

ğŸ‘‰ estimate $\,V^{\pi} (s)\,$ via Temporal-difference(TD) based approach(æ—¶é—´å·®åˆ†ç®—æ³•)
$$
    å¯¹äºæŸä¸ªtrajectory\quad \cdots,s_t,a_t,r_t,s_{t+1}\cdots \\
    å¸Œæœ›trainä¹‹åçš„networkæ»¡è¶³ V^\pi (s_t) = V^\pi (s_{t+1}) + r_t
$$
æ‰€ä»¥å°±æ”¶é›†æ•°æ®ï¼Œè¿›è¡Œå·®åˆ†æ“ä½œï¼Œè®¡ç®—å’Œ$\,r_t\,$çš„è·ç¦»ï¼Œå°±æ˜¯ä¸€ä¸ªæ‹Ÿåˆçš„é—®é¢˜ã€‚

ğŸ‘‰ Comparsion between MC and TD

![RL13](Captures\RL13.PNG "RL13")

Ex.![RL14](Captures\RL14.PNG "RL14")

å…¶ä¸­å¯¹äº$\,V^\pi (s_a)\,$çš„å€¼å‡ºç°äº†ä¸åŒ

å› ä¸ºå¯¹äºç¬¬ä¸€ä¸ªepisode,ä½¿ç”¨MCä¼šè®¤ä¸º$\,s_a influences s_b\,$,è€ŒTDä¼šè®¤ä¸ºåªæ˜¯ä¸€ä¸ªå·§åˆ(äºŒè€…å¯ä»¥è®¤ä¸ºæ˜¯ç‹¬ç«‹çš„)

### State-Action Value Funtionâ€”â€”Q Function $\,Q^\pi (s,a)\,$

actor$\,\pi\,$åœ¨state $\,a\,$,**å¼ºåˆ¶ä½¿ç”¨action** $\,a\,$,è¿™ä¸ªstateä¹‹åå¾—åˆ°çš„æ‰€æœ‰çš„rewardæ€»å’Œ

**Attï¼ï¼ï¼ï¼**ï¼šåœ¨state $\,a\,$ï¼Œactorä¸ä¸€å®šä¼šæ‰§è¡Œaction $\,a\,$ï¼Œè¿™é‡Œæ˜¯è®©actorå¼ºåˆ¶æ‰§è¡Œaction $\,a\,$

å¯¹äºä¸‹å›¾ï¼Œå·¦è¾¹æ˜¯é€šç”¨çš„ä¸€ç§è¡¨ç¤ºå½¢å¼ï¼Œå³è¾¹åªæœ‰actionæ˜¯å¯ä»¥ç©·ä¸¾çš„æ—¶å€™æ‰å¯ä»¥è¿™æ ·è¡¨ç¤º

![RL15](Captures\RL15.PNG "RL15")

ğŸ‘‰ Implement Q-Learning

![RL16](Captures\RL16.PNG "RL16")

å…ˆåˆå§‹åŒ–ä¸€ä¸ªactorï¼Œè®¡ç®—è¿™ä¸ªactorçš„Q functionï¼Œè®¡ç®—çš„æ–¹æ³•å¯ä»¥ç”¨MCæˆ–TDï¼Œ**ç„¶åä¸€å®šèƒ½å¤Ÿæ‰¾åˆ°ä¸€ä¸ªæ›´å¥½çš„actor$\,\pi^{'}\,$**ï¼Œä¹‹åä¸æ–­ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œactorå°±ä¼šè¶Šæ¥è¶Šå¥½

æ€ä¹ˆæ‰¾åˆ°æ›´å¥½çš„actorï¼Ÿ

![RL17](Captures\RL17.PNG "RL17")

ç”±äºQ-Functionæ˜¯å¯¹äºæŸä¸ªstateå¼ºåˆ¶æ‰§è¡Œactionå¾—åˆ°çš„ï¼Œè€Œactor$\,\pi\,$åœ¨æŸä¸ªstateæ‰§è¡Œçš„actionï¼Œå¯èƒ½ä¼šå’Œnew actor $\,\pi^{'}\,$åœ¨è¯¥stateæ‰§è¡Œçš„actionä¸åŒï¼Œè€Œnew actorçš„æ“ä½œæ›´ä¼˜ï¼Œ**æ‰€ä»¥è™½ç„¶æ˜¯æ›´å¥½çš„actorï¼Œä½†å¹¶ä¸æ˜¯ä¸€ä¸ªæ–°çš„networkæ¨å‡ºäº†new actorï¼Œè€Œæ˜¯Q-functionæ¨å‡ºçš„**

ä½†æ˜¯ä¸Šè¿°æ–¹æ³•ä¸é€‚ç”¨äºcontinuous action,å› ä¸ºcontinuous actionçš„actionä¸å¯ä»¥ç©·ä¸¾ï¼Œå³ä¸èƒ½å¼ºåˆ¶æ‰§è¡Œæ‰€æœ‰çš„action

### Tricks

ğŸ‘‰ Target Network

è¿ç”¨äº†TDçš„æ€æƒ³

![RL18](Captures\RL18.PNG "RL18")

ğŸ‘‰ ä¸Šå›¾æ˜¯ä¸ªé€’æ¨å¼

åœ¨è¾“å…¥æŸä¸€ä¸ªstateçš„æ—¶å€™ï¼Œè¦update networkçš„å‚æ•°ï¼Œå› ä¸ºupdateå‚æ•°ä¹‹åï¼Œä¸ä»…è¾“å‡ºå€¼ä¼šæ”¹å˜ï¼Œç›®æ ‡å€¼$\,Q^\pi (s_{t+1},\pi(s_{t+1}))\,$ä¹Ÿä¼šæ”¹å˜ï¼Œè¿™æ ·ä¼šä½¿æ¨¡å‹æ”¶æ•›çš„é€Ÿåº¦ä¸‹é™ã€‚å› æ­¤å°±é¢å¤–æ„é€ ä¸€ä¸ªtarget networkï¼Œå°†å®ƒçš„å‚æ•°ä¿æŒä¸å˜ï¼Œæ‰€ä»¥outputä¹Ÿä¿æŒä¸å˜ï¼Œåœ¨å¤šæ¬¡è°ƒæ•´å‚æ•°ä¹‹åï¼Œç”¨æ–°çš„å‚æ•°è¦†ç›–target networkï¼Œç»§ç»­å¾ªç¯çš„è®­ç»ƒ

ğŸ‘‰ Exploration

æ˜¯ä¸ºäº†é’ˆå¯¹sampleåˆ°çš„æ•°æ®ä¸å¤Ÿå¤šï¼Œactionä¸èƒ½å…¨éƒ¨çš„sampleåˆ°ï¼Œactorå‡ºç°â€œæ‡’æƒ°â€çš„æƒ…å†µï¼Œå³å½“å‰æƒ…å†µæ˜¯å±€éƒ¨æœ€ä¼˜ï¼Œå°±ä¸ç»§ç»­æ¢ç´¢æ²¡æœ‰sampleåˆ°çš„action

è®¾ç½®**Epsilon Greedy**ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º![RL19](Captures\RL19.PNG "RL19")

ğŸ‘‰ Replay Buffer

å‡å°‘å’Œç¯å¢ƒåšäº’åŠ¨çš„æ¬¡æ•°ï¼Œä¼šæ”¶é›†åˆ°å½“å‰policyä¹‹å‰çš„policyæ‰§è¡Œæ”¶é›†åˆ°çš„æ•°æ®ï¼Œå±äºOFF-POLICY

BUFFERé‡Œçš„æ•°æ®æ¥è‡ªä¸åŒçš„policyï¼Œå¾—åˆ°çš„batchçš„æ•°æ®ä¼šæ¯”è¾ƒå¤šæ ·åŒ–ï¼Œæ›´æœ‰åˆ©äºæ¨¡å‹çš„æ”¶æ•›

### Q-Learning Algorithm

![RL19](Captures\RL19.PNG "RL19")

ä¸ç¯å¢ƒäº¤äº’å¾—åˆ°å¾ˆå¤šä¸ªexperienceæ”¾å…¥bufferåï¼Œsampleæ•°æ®æŒ‰ç…§target networkçš„æ–¹æ³•åšæ‹Ÿåˆå¤„ç†