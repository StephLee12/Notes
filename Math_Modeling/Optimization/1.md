- [Optimization Methods](#optimization-methods)
  - [基本概念](#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5)
    - [一元函数极值基础](#%e4%b8%80%e5%85%83%e5%87%bd%e6%95%b0%e6%9e%81%e5%80%bc%e5%9f%ba%e7%a1%80)
    - [范数](#%e8%8c%83%e6%95%b0)
      - [向量范数](#%e5%90%91%e9%87%8f%e8%8c%83%e6%95%b0)
      - [矩阵范数](#%e7%9f%a9%e9%98%b5%e8%8c%83%e6%95%b0)
    - [梯度](#%e6%a2%af%e5%ba%a6)
    - [向量值函数的导数](#%e5%90%91%e9%87%8f%e5%80%bc%e5%87%bd%e6%95%b0%e7%9a%84%e5%af%bc%e6%95%b0)
    - [凸集和凸函数](#%e5%87%b8%e9%9b%86%e5%92%8c%e5%87%b8%e5%87%bd%e6%95%b0)
  - [线性规划](#%e7%ba%bf%e6%80%a7%e8%a7%84%e5%88%92)
  - [非线性规划](#%e9%9d%9e%e7%ba%bf%e6%80%a7%e8%a7%84%e5%88%92)
    - [MC求解非线性规划](#mc%e6%b1%82%e8%a7%a3%e9%9d%9e%e7%ba%bf%e6%80%a7%e8%a7%84%e5%88%92)
  - [最优化模型——直接搜索法和MC法](#%e6%9c%80%e4%bc%98%e5%8c%96%e6%a8%a1%e5%9e%8b%e7%9b%b4%e6%8e%a5%e6%90%9c%e7%b4%a2%e6%b3%95%e5%92%8cmc%e6%b3%95)
  - [无约束优化方法——间接法](#%e6%97%a0%e7%ba%a6%e6%9d%9f%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95%e9%97%b4%e6%8e%a5%e6%b3%95)
    - [黄金分割法(0.618法)](#%e9%bb%84%e9%87%91%e5%88%86%e5%89%b2%e6%b3%950618%e6%b3%95)
    - [最速下降法(梯度下降法)](#%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d%e6%b3%95%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95)
    - [牛顿法](#%e7%89%9b%e9%a1%bf%e6%b3%95)
    - [阻尼牛顿法](#%e9%98%bb%e5%b0%bc%e7%89%9b%e9%a1%bf%e6%b3%95)
  - [有约束优化方法](#%e6%9c%89%e7%ba%a6%e6%9d%9f%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95)
    - [外点罚函数法](#%e5%a4%96%e7%82%b9%e7%bd%9a%e5%87%bd%e6%95%b0%e6%b3%95)
    - [内点罚函数法](#%e5%86%85%e7%82%b9%e7%bd%9a%e5%87%bd%e6%95%b0%e6%b3%95)
  - [动态规划](#%e5%8a%a8%e6%80%81%e8%a7%84%e5%88%92)
  - [目标规划和多目标规划](#%e7%9b%ae%e6%a0%87%e8%a7%84%e5%88%92%e5%92%8c%e5%a4%9a%e7%9b%ae%e6%a0%87%e8%a7%84%e5%88%92)
  - [遗传算法GA](#%e9%81%97%e4%bc%a0%e7%ae%97%e6%b3%95ga)
    - [GA基本概念](#ga%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5)
    - [GA Example](#ga-example)
    - [GA Solve TSP](#ga-solve-tsp)
    - [评价GA](#%e8%af%84%e4%bb%b7ga)
  - [模拟退火(要补充)](#%e6%a8%a1%e6%8b%9f%e9%80%80%e7%81%ab%e8%a6%81%e8%a1%a5%e5%85%85)
  - [禁忌搜索(要补充)](#%e7%a6%81%e5%bf%8c%e6%90%9c%e7%b4%a2%e8%a6%81%e8%a1%a5%e5%85%85)
  - [Matlab常用的优化函数和优化工具箱(未写)](#matlab%e5%b8%b8%e7%94%a8%e7%9a%84%e4%bc%98%e5%8c%96%e5%87%bd%e6%95%b0%e5%92%8c%e4%bc%98%e5%8c%96%e5%b7%a5%e5%85%b7%e7%ae%b1%e6%9c%aa%e5%86%99)
  - [随机模拟](#%e9%9a%8f%e6%9c%ba%e6%a8%a1%e6%8b%9f)

# Optimization Methods

👉 常用的最优化方法

- 线性规划
- 整数规划
- 非线性规划
- 动态规划
- 多目标规划
- 对策论？？？

## 基本概念

### 一元函数极值基础

👉 驻点——导数为0的点。

👉 奇点——使函数连续，但导数不存在的点

👉 极值问题——求某个函数的极值，但其中的变量**收到一些条件的约束**。有约束的极值问题称为**条件极值问题**，无条件的极值问题称为**无条件极值问题**

如函数$\,f(x,y)\,$在条件$\,\varphi(x,y)=0\,$下的极值问题——$\,f(x,y)\,$是目标函数，$\,\varphi(x,y)=0\,$是约束条件

👉 等式约束下的条件极值问题的数学模型——拉格朗日乘数法

$$
    min_x f(x)\\
    s.t\quad g_i(x) = 0, i = 1,2,\cdots,m
$$

求函数$\,f(x,y)\,$在条件$\,\varphi(x,y)=0\,$下的极值，运用拉格朗日乘数法

$$
    构造函数\, F(x,y,\lambda) = f(x,y) + \lambda\varphi(x,y)\\
    令\nabla F = 0 \Rightarrow 解出(x_0,y_0)，则该点就是可能的极值点
$$

### 范数

#### 向量范数

👉 常用范数

- 1范数

$$
    ||x||_1  = |x_1| + |x_2| + \cdots + |x_n|
$$

- 2范数

$$
    ||x||_2 = (|x_1|^2 + |x_2|^2 + \cdots + |x_n|^2)^{\frac{1}{2}}
     = (x^Tx)^{\frac{1}{2}}
$$

- 无穷范数

$$
    ||x||_\infty = max_{1\leq i \leq n} |x_i|
$$

👉 范数性质

- Holder不等式

$$
    |x^T y| \leq ||x||_p ||y||_q, \frac{1}{p} + \frac{1}{q} = 1\\
    特别的，p=q=2时，是柯西-施瓦茨不等式|x^T y| \leq ||x||_2 ||y||_2
$$

#### 矩阵范数

👉 常用的矩阵范数

$$
    A\in P^{m\times n}\\
    ||A||_{m_1} = \sum_{j=1}^{n}\sum_{i=1}{m} |a_{ij}|\\
    ||A||_{m_2} = (\sum_{j=1}^{n}\sum_{i=1}{m} |a_{ij}|^2)^{\frac{1}{2}}\\
    ||A||_{m_\infty} = max_{i,j} \{|a_{ij}|\}\\
    ||A||_F = (\sum_{j=1}^{n}\sum_{i=1}{n} |a_{ij}|^2)^{\frac{1}{2}}\quad F范数
$$

### 梯度

$$
    函数f(x)存在一阶偏导数,x\in R^n,称向量\\
    \nabla f(x) = (\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},\cdots,\frac{\partial f(x)}{\partial x_n})^T\\
    为f(x)在点x处的梯度
$$

👉 梯度方向，函数值增长最快；负梯度方向，函数值增长最慢

👉 常用的梯度

$$
    常数函数c\Rightarrow \nabla c = (0,0,\cdots,0)^T = 0
$$
$$
    对b = (b_1,b_2,\cdots,b_n)^T\in R^n, x= (x_1,x_2,\cdots,x_n)^T\\
    b^Tx = b_1x_1 + b_2x_2 + \cdots + b_nx_n\\
    \nabla(b^Tx) =b
$$
$$
    \nabla (x^Tx) = \nabla(x^2_1+x^2_2+\cdots+x^2_n) = 2x
$$
$$
    若A是对称矩阵，则\\
    \nabla (x^TAx) =2Ax
$$

### 向量值函数的导数

👉 向量值函数

$$
    g(x) = (g_1(x),g_2(x),\cdots,g_m(x))^T,其中x=(x_1,x_2,\cdots,x_n)^T
$$

👉 向量值函数的一阶导数(Jacobi矩阵)

![雅可比矩阵](Captures\雅可比矩阵.PNG "雅可比矩阵")

👉 向量值函数的二阶导数(Hesse矩阵)

![Hesse矩阵](Captures\Hesse矩阵.PNG "Hesse矩阵")

### 凸集和凸函数

👉 是非线性规划的基本概念

👉 凸集

![凸集](Captures\凸集.PNG "凸集")

👉 凸函数

![凸函数](Captures\凸函数.PNG "凸函数")

凸函数的**线性运算(数乘和加法)**(但是数乘的系数必须非负，线性组合也必须非负)之后还是凸函数

凸函数的重要性

![凸函数重要性质](Captures\凸函数重要性质.PNG "凸函数重要性质")

凸函数的一阶判别条件

![凸函数一阶判断条件](Captures\凸函数一阶判断条件.PNG "凸函数一阶判断条件")

凸函数的二阶判别条件

![凸函数二阶判断条件](Captures\凸函数二阶判断条件.PNG "凸函数二阶判断条件")

👉 凸规划

$$
    设f(x)是凸函数，g(x)是凹函数，h_j(x)是线性函数，下面的规划是凸规划\\
    min \quad f(x) \quad x\in R^n\\
    s.t \quad g_i(x) \geq 0 \quad i = 1,\cdots,m\\
    h_j(x) =0 \quad j = 1,\cdots,l

$$

## 线性规划

👉 化为标准形式

👉 单纯形法

👉 Matlab求解——``linprog``

以下图为例![linprog例子](Captures\linprog例子.PNG "linprog例子")

```matlab
%目标函数中决策变量前的系数
f = [0.03 0.22 0.38 0.5 0.35];
lb = [0 0 0.1 0.2 0.05];%每一个决策变量的下界
ub = [0.3 0.25 0.3 1 0.2];%每一个决策变量的上界
A = []; b = [];
Aeq = [1 1 1 1 1]; beq = 1;
[optx,optvalue,flag] = linprog(f,A,b,Aeq,beq,lb,ub)
```

其中关于``A b Aeq beq``,![linprog例子1](Captures\linprog例子1.PNG "linprog例子1")

``linprog``是求最小值，若求最大值只需在目标函数添一个负号

若决策变量均是整数，则属于整数规划，可以在Matlab中用``intlinprog``，也可以用Lingo软件

![lingo求解](Captures\lingo求解.PNG "linpro求解")

用Lingo求解整数规划如下

```lingo
x2 + 2*x3 + 3*x4 + 5*x5 + 6*x6 <= 2000; !约束条件
5*x1 + 4*x2 + 3*x3 + 2*x4 + x5 <= 1000;
f1 = x1+ x2 + x3 + x4 + x5 + x6;  !目标函数
min = f1;
x1>=0;x2>=0;x3>=0;x4>=0;x5>=0;x6>=0;
@GIN(x1);@GIN(x2);@GIN(x3);@GIN(x4);@GIN(x5);@GIN(x6);
END
```

## 非线性规划

👉 定义：目标函数或约束条件中至少有一个是非线性函数的最优化问题

### MC求解非线性规划

见**最优化模型——直接搜索法和MC法**这一节

## 最优化模型——直接搜索法和MC法

👉 无约束，有约束都可以

👉 对于👇的模型分别使用直接搜索和MC

$$
    min \quad f(x)\\
    s.t. \quad g(x) \leq 0\\
    l_i \leq x_i \leq u_i\quad i = 1,2,\cdots,n
$$

👉 直接搜索法

要设置步长搜索

以二元函数为例

```matlab
tmin = inf ;
for x1 = l1:s1:u1 %s1=(u1-l1)/N
    for x2 = l2:s2:u2 %s2 = (u2-l2)/N
        if f(x)<tmin and g(x) <= 0:
            tmin = f(x); %存储最优值
            tx  = x; %存储决策
        endif
    endfor
endfor
```

👉 MC法

- 无约束

```matlab
N = 1e6; %投点次数
min_val = 1e5;
for i = 1:m
    for j = 1 : n %n是决策变量的维度
        r(j) = rand; %r(j)是在[0,1]均匀分布
        x(j) = l(j) + r(j) * (u(j)-l(j)); %x(j)是在[l(j),u(j)]均匀分布
    endfor
    if f(x) < min_val:
        min_val = f(x);
        min_x = x;
    endif
endfor
```

- 有约束只需在``if f(x) < min_val``加上判断是否满足约束条件即可

## 无约束优化方法——间接法

### 黄金分割法(0.618法)

👉 一种**一维搜索方法**，单变量函数最优化

👉 **单峰函数**区间上求极小值的一种方法

👉 基本思想

在搜索区间[a,b]内适当插入两点，将[a,b]分成三段，通过比较这两点的函数值，利用**单峰函数的性质**，就可以**删去最左端或最右端的一段区间**。

然后在留下的区间内再插入一点，重复👆过程。这样可以把包含极小值的区间无限缩小

### 最速下降法(梯度下降法)

👉 算法

![梯度下降法](Captures\梯度下降.PNG "梯度下降法")

👆方向就是**负梯度方向**
$$
    P^{(k)} = - \nabla f(x^{(k)})
$$

如何算最佳步长$\,\lambda^{(k)}\,$
$$
    min_\lambda f(x^{(k)}+\lambda P^{(k)})\Rightarrow min_\lambda f(x^{(k)}-\lambda \nabla f(x^{(k)})
$$
算最佳步长属于求关于$\,\lambda\,$的一元函数极值的问题，这一过程被称为一维搜索

最后梯度的长度小于某个阈值，程序停止

👉 评价

从局部看，梯度下降是朝目标函数值下降最快的方向。但是从全局看，它的收敛是比较慢的

### 牛顿法

👉 基本思想

以目标函数的一个**二次函数**作为近似，求这个二次函数的极小值点，以该点来近似原目标函数的一个局部最小值点

👉 算法

- 给定初始点$\,x^{(1)} \in R^n\,$,给定误差$\,\varepsilon > 0\,$,令$\,k=1\,$
- 由$\,\nabla^2 f(x^{(k)})d^{(k)} = - \nabla f(x^{(k)})\,$，解线性方程组得到搜索方向$\,d^{(k)}\,$
- 令$\,x^{(k+1)} = x^{(k)} + d^{(k)}\,$
- 如果$\,||d^{(k)}|| < \varepsilon\,$，则迭代终止，否则，置$\,k=k+1\,$，继续搜索迭代

Tips：$\,\nabla^2 f(x^{(k)})\,$是求Hesse矩阵

👉 评价

- 当目标函数的的梯度和Hesse矩阵易求，且能对初始点给出较好估计时，可以使用牛顿法，需要注意的是**牛顿方向不一定是下降方向**
- 当初始点远离极小值点时，牛顿法可能不收敛

### 阻尼牛顿法

👉 与牛顿法的区别：增加了沿牛顿方向进行的一维搜索，迭代公式变为
$$
    x^{(k+1)} = x^{(k)} + \lambda^kd^{(k)}
$$

👉 算法

- 给定初始点$\,x^{(1)} \in R^n\,$,给定误差$\,\varepsilon > 0\,$,令$\,k=1\,$
- 由$\,\nabla^2 f(x^{(k)})d^{(k)} = - \nabla f(x^{(k)})\,$，解线性方程组得到搜索方向$\,d^{(k)}\,$
- 从$\,x^{(k)}\,$出发，沿方向$\,d^{(k)}\,$作一维搜索求得最佳搜索步长$\,\lambda^{(k)}\,$

$$
    min_\lambda f(x^{(k)}+\lambda d^{(k)})
$$

- 令$\,x^{(k+1)} = x^{(k)} + \lambda^{(k)}d^{(k)}\,$
- 如果$\,||d^{(k)}|| < \varepsilon\,$，则迭代终止，否则，置$\,k=k+1\,$，继续搜索迭代

## 有约束优化方法

👉 主要思想：转换为无约束优化问题进行求解

### 外点罚函数法

👉 基本思想：利用目标函数和约束条件组成**辅助函数**，对违反约束的点(即位于可行域之外)在辅助函数中加入相应的“惩罚”，使得辅助函数取值很大，而对可行域内的点不予惩罚，此时辅助函数等于原问题的目标函数

👉 对于**只含等式约束的优化问题**
$$
    min f(x)\\
    s.t. h_j(x) = 0 (j= 1,2,\cdots,l)
$$
建立如下的罚函数
$$
    F(x,m_k) = f(x) + m_k \sum_{j=1}^{l} h_j^2(x)\\
    m_k是第k步迭代时很大的正整数
$$
这样就将有约束转化为了无约束的问题
$$
    min F(x, m_k)
$$
👆的最优解必然使得$\,h_j(x) \approx 0\,$

👉 对于**只含不等式约束的优化问题**
$$
    min f(x)\\
    s.t. g_i(x) \geq 0 (i =1,2,\cdots, m)
$$
构造罚函数
$$
    F(x,m_k) = f(x) + m_k \sum_{i=1}^{m} g^2_i(x)u_i(g_i)\\
    m_k是第k步迭代时很大的正整数\\
    u_i(g_i)是单位阶跃函数\\
    u_i(g_i) = \begin{cases}
        0, & g_i \geq 0\\
        1, & g_i < 0
    \end{cases}
$$

👉 对于**既含等式约束 又含不等式约束的优化问题**

构造罚函数
$$
    F(x,m_k) = f(x) + m_k P(x)\\
    P(x) = \sum_{j=1}^{l} L(h_j(x)) +  \sum_{i=1}^{m} U(g_i(x))\\
    当y= 0时，L(y) =0\\
    当y\neq0时，L(y) >0\\
    当y\geq 0时，U(y) =0\\
    当y<0时，U(y) > 0
$$
关于函数$\,L和U\,$的典型取法
$$
    L(h_j(x)) = | h_j(x)|^\beta \quad \beta \geq 1\\
    U(g_i(x)) = [max \{0,-g_i(x)\}]^\alpha \quad \alpha \geq 1
$$
其中常取$\,\alpha = 2 ,\beta =2\,$

👉 求解步骤

① 初始化

- 选定初始点$\,x^{(0)}\,$
- 初始罚因子$\,m_1\,$
- 设置罚因子放大系数$\,c>1\,$
- 置$\,k=1\,$

② 迭代

以$\,x^{(k-1)}\,$为初始点，求解无约束问题，设其极小值点为$\,x^{(k)}\,$

- 若$\,m_kP(x) < \varepsilon\,$，则$\,x^{(k)}\,$是最优解，迭代终止，否则置$\,m_{k+1}=cm_k,k=k+1\,$

👉 综上 $\,m_kP(x)\,$称为罚项，$\,m_k\,$称为罚因子，$\,F(x,m_k)\,$称为罚函数

👉 转化为无约束问题之后，就可以用无约束的求解方法进行处理。像这种通过求解无约束问题来获得约束问题最优解的方法称为**序列无约束极小化方法(SUMT)**

👉 若**可行域外性质复杂或无定义**，则外点罚函数法失效

### 内点罚函数法

👉 内点法要求迭代过程始终在可行域内进行，因此将初始点取在可行域内，并若迭代点靠近边界点时，给出的新的目标函数值迅速增大，从而使迭代点留在可行域内

👉 由于内点罚函数总是从内点出发，并保持在可行域内进行搜索，因此内点法适用于**没有等式约束的优化问题**

👉 类似于外点罚函数设计辅助函数的思路，定义**障碍函数**
$$
    F(x,r_k) = f(x) + r_kB(x)\\
    B(x)使连续函数，当x趋向可行域边界时，B(x) \rightarrow +\infty\\
    r_k是第k次迭代时的障碍因子，r_k是很小的整数
$$
$B(x)\,$的一般取法
$$
    B(x) = \sum_{i=1}^{m} \frac{1}{g_i(x)} \\
    B(x) =  -\sum_{i=1}^{m} \ln g_i(x)\\
    B(x) = \sum_{i=1}^{m} \frac{1}{g^2_i(x)}
$$
从而可以将有约束转化为无约束

👉 求解步骤和外点罚函数法类似，但是需要设置的是**障碍因子的缩小系数**

👉 内点法和外点法的收敛情况与**罚因子的选取关系很大**

## 动态规划

👉 是用来解决**多阶段决策过程最优化的一种数量方法**。可以把一个$\,n\,$维决策问题变换为几个一维最优化问题

👉 例子

- 最短路
- 投资分配
- 背包问题(也可归为整数规划)
- 排序问题

👉 用LINGO求解最短路

## 目标规划和多目标规划

## 遗传算法GA

### GA基本概念

👉 思想

- 种群中个体的选择
- 种群中交叉繁殖
- 种群中个体的变异

👆反复执行，个体逐渐优化

### GA Example

👉 Ex

$$
    求解方程\quad x^{10} + e^x  =100(x>0)
$$

将方程求解问题转化为生存问题——将区间[0,10](解一定在[0,10]内，当然也可以缩小区间)划分成若干小区间，设想每个小区间为一个生物个体，$\,|x^{10} + e^x - 100|\,$最小的个体有最好的生存能力,该个体即为解

- 初始化

👉 个体编码

若使用二进制编码，假设要求的精度是小数点后m位，区间的长度是n，则至少要把区间$\,n\times 10^m\,$份，之后求与此数最接近的2的$\,p\,$次幂，$\,p\,$就是二进制串的位数

假定本题要求小数点后两位，至少要把区间划分$\,10\times 10^2 = 1000\,$份，所以要把区间划分为$\,2^{10}=1024\,$，二进制串的位数为10

👉 种群初始化

随机生成任意数量的10位二进制串

- 选择

👉 定义适应度函数

这里是以目标函数的倒数作为适应度函数

$$
    f = \frac{1}{|x^{10}+e^x-100|}
$$

👉 选择适应度较大的个体

**轮盘赌**：将所有个体的适应度求和，得到总适应度。每一个体被选择到的概率为**该个体适应度/总适应度**

代码实现——产生[0,1]的随机数选择个体

- 交叉

选中的优势个体进行交叉，这里采用**单点交叉：在编码串中只随机设置一个交叉点，以该点为分界，交换部分染色体**

👉 交叉算子有好几种

- 变异

在个体中的某几位的值发生改变

👉 变异的方法也有好几种

👉 交叉和变异操作都以一定概率进行

👉 反复进行选择，交叉，变异并记录最优个体。

👉 程序结束的判定

- 根据循环次数
- 根据最大适应度
- 根据种群中相同个体数与总个体数的比值

### GA Solve TSP

- 初始化

取长度为N的数字串，串中数字互不重复，取值范围为[1,N]之间的整数，每一个数字串代表一个个体，个体中数字出现的位置是路径中城市出现的顺序

- 选择

适应度函数——路径长度的导数

- 交叉

TSP的交叉算子与普通的不同——要保证生成的解是有效解

👉 在一个父个体中随机选取一段子串A，在另一个父个体中将字串A出现的数字去掉形成串B，AB就是一个新的字串

- 变异

👉 常用的变异操作——随机选取两个相邻位置的数字，交换顺序

### 评价GA

👉 GA各步骤的作用

![GA](Captures\GA.PNG "GA")

👉 GA的评价

优点

- 全局收敛性(依概率收敛)
- 泛化能力很强

缺点

- 得到的解是**近似的数值解**
- 对于经典数学可以解决的问题，效率很低

## 模拟退火(要补充)

## 禁忌搜索(要补充)

## Matlab常用的优化函数和优化工具箱(未写)

## 随机模拟
